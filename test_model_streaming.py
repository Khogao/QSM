"""
Test Vietnamese Law Model - Faster Version
With longer timeout and streaming support
"""
import requests
import json
import time

LM_STUDIO_URL = "http://localhost:1234/v1/chat/completions"
MODEL_NAME = "chatbot_vietnamese_law_qwen"

def test_streaming():
    """Test with streaming for better responsiveness"""
    
    print("ðŸš€ TEST MODEL Vá»šI STREAMING")
    print("=" * 60)
    
    question = "Luáº­t Ä‘áº§u tÆ° 2020 cÃ³ Ä‘iá»u khoáº£n nÃ o quan trá»ng?"
    
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {
                "role": "system",
                "content": "Báº¡n lÃ  chuyÃªn gia tÆ° váº¥n phÃ¡p luáº­t Viá»‡t Nam. Tráº£ lá»i ngáº¯n gá»n."
            },
            {
                "role": "user",
                "content": question
            }
        ],
        "temperature": 0.7,
        "max_tokens": 300,
        "stream": True
    }
    
    print(f"\nðŸ“ Question: {question}\n")
    print("ðŸ“¨ Response: ", end="", flush=True)
    
    start_time = time.time()
    total_tokens = 0
    full_response = ""
    
    try:
        with requests.post(
            LM_STUDIO_URL,
            json=payload,
            stream=True,
            timeout=180
        ) as response:
            
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    if line_text.startswith('data: '):
                        data_str = line_text[6:]
                        if data_str == '[DONE]':
                            break
                        
                        try:
                            data = json.loads(data_str)
                            if 'choices' in data and len(data['choices']) > 0:
                                delta = data['choices'][0].get('delta', {})
                                content = delta.get('content', '')
                                if content:
                                    print(content, end="", flush=True)
                                    full_response += content
                                    total_tokens += 1
                        except json.JSONDecodeError:
                            pass
        
        elapsed = time.time() - start_time
        tokens_per_sec = total_tokens / elapsed if elapsed > 0 else 0
        
        print(f"\n\n{'=' * 60}")
        print(f"âœ… Time: {elapsed:.2f}s")
        print(f"âœ… Tokens: ~{total_tokens}")
        print(f"âœ… Speed: ~{tokens_per_sec:.1f} tokens/sec")
        print(f"âœ… Response length: {len(full_response)} chars")
        
        # Check speed
        if tokens_per_sec < 10:
            print(f"\nâš ï¸ Cáº¢NH BÃO: Tá»‘c Ä‘á»™ cháº­m ({tokens_per_sec:.1f} tok/s)")
            print("ðŸ’¡ Khuyáº¿n nghá»‹:")
            print("  1. Kiá»ƒm tra GPU layers trong LM Studio")
            print("  2. Giáº£m context length")
            print("  3. Thá»­ quantization nhá» hÆ¡n (Q4_K_M)")
        elif tokens_per_sec < 50:
            print(f"\nâš ï¸ Tá»‘c Ä‘á»™ cháº¥p nháº­n Ä‘Æ°á»£c nhÆ°ng cÃ³ thá»ƒ tá»‘i Æ°u")
            print(f"ðŸ’¡ NÃªn Ä‘áº¡t 60-100 tok/s vá»›i Vulkan")
        else:
            print(f"\nâœ… Tá»‘c Ä‘á»™ Tá»T! ({tokens_per_sec:.1f} tok/s)")
        
        return True
        
    except requests.exceptions.Timeout:
        print("\nâŒ TIMEOUT! Model pháº£n há»“i quÃ¡ cháº­m")
        return False
    except Exception as e:
        print(f"\nâŒ ERROR: {e}")
        return False

def check_gpu_status():
    """Check if GPU is being used"""
    print("\nðŸ” KIá»‚M TRA Cáº¤U HÃŒNH GPU")
    print("=" * 60)
    print("ðŸ’¡ Má»Ÿ LM Studio vÃ  kiá»ƒm tra:")
    print("  â€¢ Settings > Hardware > GPU Acceleration")
    print("  â€¢ Model Settings > GPU Layers (nÃªn Ä‘á»ƒ Auto hoáº·c 99)")
    print("  â€¢ Kiá»ƒm tra VRAM usage trong Task Manager")
    print("=" * 60)

if __name__ == "__main__":
    check_gpu_status()
    test_streaming()
