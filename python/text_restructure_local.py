"""
üîÑ Quicord v3.2 - Local LLM Text Restructuring
üéØ Use local LLM to reorder OCR paragraphs without API costs

Supported models:
1. Qwen2.5-7B-Instruct (Best Vietnamese support)
2. Llama-3.2-3B-Instruct (Fastest, smallest)
3. Phi-3-mini-4k (Best document understanding)

Requirements:
- transformers>=4.40.0
- torch>=2.0.0
- accelerate>=0.20.0
"""

import sys
import os
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional

# Check dependencies
try:
    import torch
    from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
except ImportError:
    print("‚ùå ERROR: Missing dependencies!")
    print("üì¶ Install: pip install transformers torch accelerate")
    sys.exit(1)


class LocalLLMRestructurer:
    """Use local LLM to restructure OCR text"""
    
    # Supported models with Vietnamese document restructuring
    MODELS = {
        "qwen": {
            "name": "Qwen/Qwen2.5-7B-Instruct",
            "size": "~8GB RAM",
            "speed": "Medium",
            "vietnamese": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
            "recommended": True,
            "description": "Best for Vietnamese documents, excellent context understanding"
        },
        "llama": {
            "name": "meta-llama/Llama-3.2-3B-Instruct",
            "size": "~4GB RAM",
            "speed": "Fast",
            "vietnamese": "‚≠ê‚≠ê‚≠ê‚≠ê",
            "recommended": False,
            "description": "Smallest & fastest, good Vietnamese support"
        },
        "phi": {
            "name": "microsoft/Phi-3-mini-4k-instruct",
            "size": "~4GB RAM",
            "speed": "Fast",
            "vietnamese": "‚≠ê‚≠ê‚≠ê‚≠ê",
            "recommended": False,
            "description": "Best for document tasks, optimized for CPU"
        }
    }
    
    def __init__(self, model_name: str = "qwen", device: str = "auto"):
        """
        Initialize local LLM restructurer
        
        Args:
            model_name: Model to use (qwen/llama/phi)
            device: Device to use (auto/cpu/cuda)
        """
        self.model_name = model_name
        self.device = device
        self.model = None
        self.tokenizer = None
        self.pipeline = None
        
        # Model cache directory
        self.cache_dir = Path.home() / ".cache" / "quicord_models"
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"ü§ñ Initializing {self.MODELS[model_name]['name']}...")
        self._load_model()
    
    def _load_model(self):
        """Load model and tokenizer"""
        model_info = self.MODELS[self.model_name]
        model_id = model_info["name"]
        
        try:
            print(f"üì¶ Loading tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_id,
                cache_dir=str(self.cache_dir),
                trust_remote_code=True
            )
            
            print(f"üß† Loading model (this may take a few minutes first time)...")
            
            # Determine device
            if self.device == "auto":
                device = "cuda" if torch.cuda.is_available() else "cpu"
            else:
                device = self.device
            
            print(f"üíª Using device: {device.upper()}")
            
            # Load model with appropriate settings
            if device == "cuda":
                # Use GPU with float16 for speed
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    cache_dir=str(self.cache_dir),
                    torch_dtype=torch.float16,
                    device_map="auto",
                    trust_remote_code=True
                )
            else:
                # Use CPU with float32
                self.model = AutoModelForCausalLM.from_pretrained(
                    model_id,
                    cache_dir=str(self.cache_dir),
                    torch_dtype=torch.float32,
                    trust_remote_code=True
                )
                self.model.to(device)
            
            # Create pipeline
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device=0 if device == "cuda" else -1
            )
            
            print(f"‚úÖ Model loaded successfully!")
            print(f"üìä Size: {model_info['size']}")
            print(f"‚ö° Speed: {model_info['speed']}")
            print(f"üáªüá≥ Vietnamese: {model_info['vietnamese']}")
            
        except Exception as e:
            print(f"‚ùå ERROR loading model: {e}")
            print(f"üí° Try: pip install transformers torch accelerate --upgrade")
            raise
    
    def restructure(
        self,
        ocr_text: str,
        doc_type: str = "contract",
        max_length: int = 4096,
        temperature: float = 0.1
    ) -> Tuple[str, Dict]:
        """
        Restructure OCR text using local LLM
        
        Args:
            ocr_text: Raw OCR text with interleaved paragraphs
            doc_type: Document type (contract/invoice/blueprint/report)
            max_length: Maximum output length
            temperature: Sampling temperature (0.1 = deterministic)
        
        Returns:
            (restructured_text, metadata)
        """
        # Build prompt based on document type
        prompt = self._build_prompt(ocr_text, doc_type)
        
        print(f"üîÑ Restructuring {doc_type}...")
        print(f"üìù Input length: {len(ocr_text)} chars")
        
        try:
            # Generate with LLM
            result = self.pipeline(
                prompt,
                max_new_tokens=max_length,
                temperature=temperature,
                do_sample=temperature > 0,
                top_p=0.95,
                repetition_penalty=1.1
            )
            
            # Extract generated text
            generated = result[0]["generated_text"]
            
            # Remove prompt from output
            if prompt in generated:
                restructured = generated.replace(prompt, "").strip()
            else:
                restructured = generated.strip()
            
            # Extract restructured text (between markers)
            if "### ƒê·∫¶U RA ###" in restructured:
                parts = restructured.split("### ƒê·∫¶U RA ###")
                if len(parts) > 1:
                    restructured = parts[1].strip()
            
            print(f"‚úÖ Output length: {len(restructured)} chars")
            
            # Metadata
            metadata = {
                "model": self.MODELS[self.model_name]["name"],
                "doc_type": doc_type,
                "input_length": len(ocr_text),
                "output_length": len(restructured),
                "temperature": temperature
            }
            
            return restructured, metadata
            
        except Exception as e:
            print(f"‚ùå ERROR during restructuring: {e}")
            raise
    
    def _build_prompt(self, ocr_text: str, doc_type: str) -> str:
        """Build prompt for LLM based on document type"""
        
        # Vietnamese document type names
        doc_names = {
            "contract": "H·ª¢P ƒê·ªíNG",
            "invoice": "H√ìA ƒê∆†N",
            "blueprint": "B·∫¢N V·∫º K·ª∏ THU·∫¨T",
            "report": "B√ÅO C√ÅO"
        }
        
        doc_name = doc_names.get(doc_type, "T√ÄI LI·ªÜU")
        
        # Base prompt (Vietnamese)
        prompt = f"""B·∫°n l√† chuy√™n gia s·∫Øp x·∫øp l·∫°i vƒÉn b·∫£n ti·∫øng Vi·ªát t·ª´ OCR.

### NHI·ªÜM V·ª§ ###
S·∫Øp x·∫øp l·∫°i c√°c ƒëo·∫°n vƒÉn trong vƒÉn b·∫£n {doc_name} d∆∞·ªõi ƒë√¢y theo th·ª© t·ª± logic ƒë√∫ng.

### QUY T·∫ÆC ###
1. TUY·ªÜT ƒê·ªêI KH√îNG th√™m ch·ªØ m·ªõi
2. TUY·ªÜT ƒê·ªêI KH√îNG x√≥a/b·ªè b·∫•t k·ª≥ ch·ªØ n√†o
3. CH·ªà s·∫Øp x·∫øp l·∫°i th·ª© t·ª± c√°c ƒëo·∫°n vƒÉn
4. Gi·ªØ NGUY√äN V·∫∏N n·ªôi dung g·ªëc
5. Hi·ªÉu ng·ªØ c·∫£nh ƒë·ªÉ s·∫Øp x·∫øp ƒë√∫ng c·∫•u tr√∫c t√†i li·ªáu

### C·∫§U TR√öC {doc_name.upper()} ###
"""

        # Add document-specific structure guidelines
        if doc_type == "contract":
            prompt += """
- ƒê·∫ßu t√†i li·ªáu: Ti√™u ƒë·ªÅ, ng√†y th√°ng, c∆° quan ban h√†nh
- Ph·∫ßn m·ªü ƒë·∫ßu: CƒÉn c·ª© ph√°p l√Ω, c√°c b√™n tham gia
- N·ªôi dung ch√≠nh: C√°c ƒëi·ªÅu kho·∫£n (ƒêi·ªÅu 1, ƒêi·ªÅu 2...)
- Quy·ªÅn v√† nghƒ©a v·ª• c·ªßa c√°c b√™n
- ƒêi·ªÅu kho·∫£n chung
- Ph·∫ßn cu·ªëi: Ch·ªØ k√Ω, ƒë·∫°i di·ªán ph√°p lu·∫≠t
"""
        elif doc_type == "invoice":
            prompt += """
- ƒê·∫ßu h√≥a ƒë∆°n: Ti√™u ƒë·ªÅ, m√£ s·ªë thu·∫ø, ng√†y th√°ng
- Th√¥ng tin ng∆∞·ªùi b√°n/ng∆∞·ªùi mua
- B·∫£ng chi ti·∫øt s·∫£n ph·∫©m/d·ªãch v·ª•
- T·ªïng ti·ªÅn, thu·∫ø GTGT
- Ph·∫ßn cu·ªëi: Ch·ªØ k√Ω, con d·∫•u
"""
        elif doc_type == "blueprint":
            prompt += """
- ƒê·∫ßu b·∫£n v·∫Ω: Ti√™u ƒë·ªÅ, m√£ s·ªë b·∫£n v·∫Ω, t·ª∑ l·ªá
- Th√¥ng tin d·ª± √°n
- N·ªôi dung b·∫£n v·∫Ω ch√≠nh
- B·∫£ng ch√∫ th√≠ch, k√Ω hi·ªáu
- Ph·∫ßn cu·ªëi: Ng∆∞·ªùi v·∫Ω, ng∆∞·ªùi duy·ªát
"""
        else:
            prompt += """
- ƒê·∫ßu t√†i li·ªáu: Ti√™u ƒë·ªÅ, ng√†y th√°ng
- N·ªôi dung ch√≠nh: C√°c ph·∫ßn, ƒëo·∫°n vƒÉn theo logic
- Ph·∫ßn cu·ªëi: K·∫øt lu·∫≠n, ch·ªØ k√Ω
"""

        prompt += f"""
### VƒÇN B·∫¢N OCR G·ªêC ###
{ocr_text}

### ƒê·∫¶U RA ###
VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c s·∫Øp x·∫øp l·∫°i theo ƒë√∫ng c·∫•u tr√∫c (gi·ªØ nguy√™n n·ªôi dung):

"""
        
        return prompt
    
    @staticmethod
    def list_models():
        """List all available models with details"""
        print("\nüìã AVAILABLE LOCAL MODELS:\n")
        
        for key, info in LocalLLMRestructurer.MODELS.items():
            print(f"{'‚≠ê ' if info['recommended'] else '   '}{key.upper()}: {info['name']}")
            print(f"    Size: {info['size']}")
            print(f"    Speed: {info['speed']}")
            print(f"    Vietnamese: {info['vietnamese']}")
            print(f"    {info['description']}")
            print()
    
    @staticmethod
    def check_requirements():
        """Check if system meets requirements"""
        print("\nüîç SYSTEM CHECK:\n")
        
        # Check Python version
        py_version = sys.version_info
        print(f"Python: {py_version.major}.{py_version.minor}.{py_version.micro}", end=" ")
        if py_version >= (3, 8):
            print("‚úÖ")
        else:
            print("‚ùå (Need 3.8+)")
        
        # Check PyTorch
        try:
            import torch
            print(f"PyTorch: {torch.__version__} ‚úÖ")
            
            # Check CUDA
            if torch.cuda.is_available():
                print(f"CUDA: Available (GPU: {torch.cuda.get_device_name(0)}) ‚ö°")
                print(f"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
            else:
                print(f"CUDA: Not available (will use CPU) üíª")
        except ImportError:
            print("PyTorch: Not installed ‚ùå")
        
        # Check Transformers
        try:
            import transformers
            print(f"Transformers: {transformers.__version__} ‚úÖ")
        except ImportError:
            print("Transformers: Not installed ‚ùå")
        
        # Check RAM
        try:
            import psutil
            ram_gb = psutil.virtual_memory().total / 1e9
            print(f"RAM: {ram_gb:.1f} GB", end=" ")
            if ram_gb >= 16:
                print("‚úÖ (Can run all models)")
            elif ram_gb >= 8:
                print("‚ö†Ô∏è (Can run small models)")
            else:
                print("‚ùå (Need 8GB+ for LLM)")
        except ImportError:
            print("RAM: Unknown (install psutil to check)")
        
        print()


def main():
    """Demo usage"""
    import argparse
    
    parser = argparse.ArgumentParser(description="Local LLM Text Restructuring")
    parser.add_argument("--list-models", action="store_true", help="List available models")
    parser.add_argument("--check", action="store_true", help="Check system requirements")
    parser.add_argument("--model", default="qwen", help="Model to use (qwen/llama/phi)")
    parser.add_argument("--input", help="Input file (OCR text)")
    parser.add_argument("--output", help="Output file (restructured text)")
    parser.add_argument("--type", default="contract", help="Document type")
    
    args = parser.parse_args()
    
    if args.list_models:
        LocalLLMRestructurer.list_models()
        return
    
    if args.check:
        LocalLLMRestructurer.check_requirements()
        return
    
    if not args.input:
        print("‚ùå ERROR: --input required")
        parser.print_help()
        return
    
    # Load OCR text
    input_path = Path(args.input)
    if not input_path.exists():
        print(f"‚ùå ERROR: File not found: {input_path}")
        return
    
    ocr_text = input_path.read_text(encoding="utf-8")
    
    # Initialize restructurer
    restructurer = LocalLLMRestructurer(model_name=args.model)
    
    # Restructure
    restructured, metadata = restructurer.restructure(ocr_text, doc_type=args.type)
    
    # Save output
    if args.output:
        output_path = Path(args.output)
    else:
        output_path = input_path.with_stem(f"{input_path.stem}_restructured")
    
    output_path.write_text(restructured, encoding="utf-8")
    print(f"‚úÖ Saved to: {output_path}")
    
    # Print metadata
    print(f"\nüìä METADATA:")
    print(json.dumps(metadata, indent=2, ensure_ascii=False))


if __name__ == "__main__":
    main()
