"""
üì¶ Quicord v3.2 - Optional LLM Installer
Install local LLM models for AI text restructuring (OPTIONAL FEATURE)

This script allows users to choose if they want AI text restructuring.
Models are NOT bundled with app to keep it small (~500MB core).
"""

import sys
import os
from pathlib import Path

def clear_screen():
    """Clear terminal screen"""
    os.system('cls' if os.name == 'nt' else 'clear')


def show_header():
    """Show installer header"""
    print("=" * 80)
    print("üì¶ QUICORD v3.2 - OPTIONAL LLM INSTALLER")
    print("=" * 80)
    print("ü§ñ Install local AI model for text restructuring (OPTIONAL)")
    print("üìù Fix paragraph ordering in OCR output using AI")
    print("üîí 100% local, private, free (after download)")
    print("=" * 80)
    print()


def show_models():
    """Show available models"""
    print("üîç AVAILABLE MODELS:")
    print()
    
    models = [
        {
            "id": "1",
            "name": "Qwen2.5-3B-Instruct",
            "badge": "üèÜ BEST",
            "size": "~3GB",
            "context": "32K tokens",
            "quality": "87%",
            "speed": "Fast",
            "vietnamese": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
            "desc": "Latest Qwen 2.5, handles ALL Vietnamese contracts"
        },
        {
            "id": "2",
            "name": "Qwen2.5-1.5B-Instruct",
            "badge": "‚ö° FASTEST",
            "size": "~1.5GB",
            "context": "32K tokens",
            "quality": "82%",
            "speed": "Very Fast",
            "vietnamese": "‚≠ê‚≠ê‚≠ê‚≠ê",
            "desc": "Tiny & fast, perfect for low-end PCs (8GB RAM)"
        },
        {
            "id": "3",
            "name": "Qwen2.5-7B-Instruct",
            "badge": "üíé HIGHEST",
            "size": "~8GB",
            "context": "32K tokens",
            "quality": "88%",
            "speed": "Medium",
            "vietnamese": "‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê",
            "desc": "Best quality but large (needs 16GB RAM)"
        },
        {
            "id": "4",
            "name": "Gemma-2-2B",
            "badge": "üîµ GOOGLE",
            "size": "~2GB",
            "context": "8K tokens",
            "quality": "84%",
            "speed": "Fast",
            "vietnamese": "‚≠ê‚≠ê‚≠ê‚≠ê",
            "desc": "Google's model, small & good"
        }
    ]
    
    for model in models:
        print(f"{model['badge']} [{model['id']}] {model['name']}")
        print(f"    Size: {model['size']} | Context: {model['context']} | Quality: {model['quality']}")
        print(f"    Speed: {model['speed']} | Vietnamese: {model['vietnamese']}")
        print(f"    {model['desc']}")
        print()
    
    print("[0] ‚è≠Ô∏è  SKIP - Don't install LLM (use manual restructuring)")
    print()


def check_dependencies():
    """Check if dependencies are installed"""
    print("üîç Checking dependencies...")
    print()
    
    missing = []
    
    try:
        import torch
        print(f"‚úÖ PyTorch: {torch.__version__}")
        
        if torch.cuda.is_available():
            print(f"‚úÖ CUDA: Available (GPU: {torch.cuda.get_device_name(0)})")
        else:
            print(f"‚ÑπÔ∏è  CUDA: Not available (will use CPU)")
    except ImportError:
        print("‚ùå PyTorch: Not installed")
        missing.append("torch")
    
    try:
        import transformers
        print(f"‚úÖ Transformers: {transformers.__version__}")
    except ImportError:
        print("‚ùå Transformers: Not installed")
        missing.append("transformers")
    
    try:
        import accelerate
        print(f"‚úÖ Accelerate: {accelerate.__version__}")
    except ImportError:
        print("‚ùå Accelerate: Not installed")
        missing.append("accelerate")
    
    print()
    
    if missing:
        print(f"‚ùå Missing dependencies: {', '.join(missing)}")
        print()
        print("üì¶ Install them first:")
        print(f"   pip install {' '.join(missing)}")
        print()
        return False
    
    return True


def install_model(model_choice):
    """Install selected model"""
    model_map = {
        "1": ("Qwen/Qwen2.5-3B-Instruct", "qwen3b", "~3GB"),
        "2": ("Qwen/Qwen2.5-1.5B-Instruct", "qwen1.5b", "~1.5GB"),
        "3": ("Qwen/Qwen2.5-7B-Instruct", "qwen7b", "~8GB"),
        "4": ("google/gemma-2-2b-instruct", "gemma2b", "~2GB")
    }
    
    if model_choice not in model_map:
        print("‚ùå Invalid choice")
        return False
    
    model_id, model_key, model_size = model_map[model_choice]
    
    print("=" * 80)
    print(f"üì¶ INSTALLING: {model_id}")
    print("=" * 80)
    print(f"üìä Size: {model_size}")
    print(f"üìç Location: ~/.cache/quicord_models/")
    print(f"‚è≥ This will take 2-10 minutes (depending on internet speed)")
    print()
    
    confirm = input("Continue? (yes/no): ").strip().lower()
    if confirm not in ["yes", "y"]:
        print("‚ùå Cancelled")
        return False
    
    print()
    print("üì• Downloading model...")
    print("‚è≥ Please wait...\n")
    
    try:
        from transformers import AutoTokenizer, AutoModelForCausalLM
        
        cache_dir = Path.home() / ".cache" / "quicord_models"
        cache_dir.mkdir(parents=True, exist_ok=True)
        
        # Download tokenizer
        print("üì• Downloading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_id,
            cache_dir=str(cache_dir),
            trust_remote_code=True
        )
        print("‚úÖ Tokenizer downloaded!")
        
        # Download model
        print(f"üì• Downloading model ({model_size})...")
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            cache_dir=str(cache_dir),
            torch_dtype="auto",
            device_map="auto",
            trust_remote_code=True
        )
        print("‚úÖ Model downloaded!")
        
        print()
        print("=" * 80)
        print("‚úÖ INSTALLATION COMPLETE!")
        print("=" * 80)
        print(f"üìç Model cached at: {cache_dir}")
        print(f"üíæ Disk usage: {model_size}")
        print()
        print("üéâ You can now use AI text restructuring in Quicord!")
        print()
        print("üìñ Usage:")
        print(f"   python python/text_restructure_local.py --model {model_key} --input file.txt")
        print()
        
        return True
        
    except Exception as e:
        print()
        print(f"‚ùå ERROR: {e}")
        print()
        print("üí° TROUBLESHOOTING:")
        print("1. Check internet connection")
        print("2. Check disk space (need ~10GB free)")
        print("3. Try again later (HuggingFace may be down)")
        return False


def main():
    """Main installer"""
    clear_screen()
    show_header()
    
    # Check dependencies first
    if not check_dependencies():
        print("‚ùå Please install dependencies first, then run this script again.")
        return
    
    print()
    print("=" * 80)
    show_models()
    print("=" * 80)
    
    print("üí° RECOMMENDATION:")
    print("   Option 1 (Qwen2.5-3B) is BEST for most users!")
    print("   - 3GB download, 32K context, 87% quality")
    print("   - Handles ALL Vietnamese contracts")
    print()
    
    choice = input("Your choice (0-4): ").strip()
    
    if choice == "0":
        print()
        print("‚è≠Ô∏è  SKIPPED - No LLM installed")
        print("‚ÑπÔ∏è  You can install it later by running:")
        print("   python python/install_llm_optional.py")
        print()
        print("üìù Quicord will work without LLM (manual text restructuring)")
        return
    
    print()
    success = install_model(choice)
    
    if success:
        print("üéâ SUCCESS! AI text restructuring is now available!")
    else:
        print("‚ùå Installation failed. You can try again later.")
    
    print()
    input("Press Enter to exit...")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\n‚ùå Cancelled by user")
    except Exception as e:
        print(f"\n\n‚ùå UNEXPECTED ERROR: {e}")
        import traceback
        traceback.print_exc()
