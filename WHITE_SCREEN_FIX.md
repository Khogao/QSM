# üîß WHITE SCREEN FIX - QSM on X220

## üéØ V·∫•n ƒë·ªÅ: C·ª≠a s·ªï tr·∫Øng khi m·ªü QSM

B·∫°n ƒëang g·∫∑p v·∫•n ƒë·ªÅ ph·ªï bi·∫øn: **QSM UI kh√¥ng render** m·∫∑c d√π app ƒë√£ ch·∫°y.

---

## ‚úÖ QUICK FIX (5 ph√∫t)

### **B∆∞·ªõc 1: Verify Ollama**

```bash
# Check Ollama running
curl http://localhost:11434/api/tags

# Should see list of models
# If not, start Ollama:
ollama serve &
```

### **B∆∞·ªõc 2: Launch QSM Debug Mode**

```bash
cd /home/phi/QSM
./debug-qsm.sh
```

Debug script s·∫Ω:
- ‚úÖ Check Ollama connection
- ‚úÖ Test model response
- ‚úÖ Show console logs
- ‚úÖ Save logs to `/tmp/qsm-debug.log`

### **B∆∞·ªõc 3: Khi QSM m·ªü ra (white screen)**

**M·ªü DevTools:** Press `Ctrl+Shift+I` (ho·∫∑c `F12`)

**Check Console tab ƒë·ªÉ xem l·ªói:**
- ‚ùå `Failed to fetch` ‚Üí Ollama not connected
- ‚ùå `Cannot read property 'xxx' of undefined` ‚Üí Missing config
- ‚ùå `Network error` ‚Üí Backend kh√¥ng response

### **B∆∞·ªõc 4: Configure Ollama trong QSM**

1. **N·∫øu UI hi·ªán Settings icon** (‚öôÔ∏è):
   - Click Settings (g√≥c ph·∫£i tr√™n)
   - T√¨m "Ollama Configuration"
   - URL: `http://localhost:11434`
   - Model: `llama3.2` ho·∫∑c `qwen2.5:3b`
   - Click Save

2. **N·∫øu kh√¥ng th·∫•y UI** (ho√†n to√†n tr·∫Øng):
   - Check DevTools Console (Ctrl+Shift+I)
   - Look for error messages
   - Copy error v√† b√°o l·∫°i

---

## üîç COMMON ISSUES

### **Issue 1: "Failed to fetch" trong Console**

**Nguy√™n nh√¢n:** Ollama kh√¥ng ch·∫°y ho·∫∑c sai port

**Fix:**
```bash
# Start Ollama
ollama serve &

# Verify
curl http://localhost:11434/api/tags

# Should see: {"models":[...]}
```

### **Issue 2: "Model not found"**

**Nguy√™n nh√¢n:** Model ch∆∞a pull v·ªÅ

**Fix:**
```bash
# Pull recommended model
ollama pull llama3.2:3b

# Or use existing model
ollama list
```

### **Issue 3: Blank white screen, kh√¥ng c√≥ error**

**Nguy√™n nh√¢n:** Frontend build issue ho·∫∑c Electron render process crash

**Fix 1 - Rebuild QSM:**
```bash
cd /home/phi/QSM
npm run build:linux
```

**Fix 2 - Clear cache:**
```bash
rm -rf ~/.config/QSM/
rm -rf ~/.cache/QSM/
```

**Fix 3 - Check GPU acceleration:**
```bash
# Launch with software rendering
QSM - QueryMaster-1.0.0.AppImage --disable-gpu
```

### **Issue 4: "llama runner process has terminated"**

**Nguy√™n nh√¢n:** Model crash (c√≥ th·ªÉ do RAM kh√¥ng ƒë·ªß)

**Fix:**
```bash
# Use smaller model
ollama pull llama3.2:1b  # Only 700MB

# Or qwen2.5:3b (better balance)
ollama pull qwen2.5:3b
```

---

## üß™ DIAGNOSTIC COMMANDS

```bash
# 1. Check if QSM process running
ps aux | grep QSM

# 2. Check Ollama status
curl http://localhost:11434/api/version

# 3. Test model inference
curl -X POST http://localhost:11434/api/chat \
  -d '{"model":"llama3.2","messages":[{"role":"user","content":"test"}],"stream":false}'

# 4. Check QSM logs
cat /tmp/qsm-debug.log

# 5. Check Ollama logs
cat /tmp/ollama.log

# 6. Check system resources
free -h  # RAM usage
df -h    # Disk space
```

---

## üìä WORKING CONFIGURATION (X220)

**System:**
- RAM: 11GB (QSM needs ~500MB, Ollama+model needs 2-4GB)
- Disk: 68GB free (models take 1-5GB each)
- CPU: 4 cores (should be OK for 3B models)

**Recommended Setup:**
```bash
# Ollama
URL: http://localhost:11434
Model: llama3.2:3b (2GB) or qwen2.5:3b (1.9GB)

# QSM Settings (when UI loads)
Chunk Size: 500 chars
Overlap: 50 chars
Top K Results: 5
Temperature: 0.7
```

---

## üéØ NEXT STEPS

### **Step 1: Run Debug Mode**
```bash
cd /home/phi/QSM
./debug-qsm.sh
```

### **Step 2: Open DevTools khi white screen**
- Press: `Ctrl+Shift+I`
- Go to Console tab
- Copy any error messages

### **Step 3: Report back v·ªõi:**
1. Screenshot c·ªßa Console errors
2. Output c·ªßa `./debug-qsm.sh`
3. Last 20 lines c·ªßa `/tmp/qsm-debug.log`

---

## üí° ALTERNATIVES (N·∫øu QSM kh√¥ng work)

### **Option 1: Run QSM in dev mode**
```bash
cd /home/phi/QSM
npm run dev
```
This will:
- Show more detailed errors
- Hot reload on changes
- Easier to debug

### **Option 2: Use web version**
```bash
cd /home/phi/QSM
npm run build
npm run preview
```
Open: http://localhost:4173

### **Option 3: Check if it's GPU issue**
```bash
# Launch without GPU acceleration
cd /home/phi/QSM/release
./"QSM - QueryMaster-1.0.0.AppImage" --disable-gpu --no-sandbox
```

---

## üÜò STILL NOT WORKING?

Run full diagnostic:
```bash
cd /home/phi/QSM
./debug-qsm.sh > /tmp/full-diagnostic.log 2>&1

# Then check:
cat /tmp/full-diagnostic.log
cat /tmp/qsm-debug.log
```

Copy output v√† b√°o l·∫°i!

---

## ‚úÖ SUCCESS INDICATORS

Khi QSM work properly, b·∫°n s·∫Ω th·∫•y:

1. **Main Window:** 
   - Left sidebar: "Documents", "Upload Files"
   - Center: Query input box
   - Right: Settings icon

2. **Console (Ctrl+Shift+I):**
   - No red errors
   - Maybe some warnings (OK)
   - Network requests to Ollama (http://localhost:11434)

3. **After upload document:**
   - Progress bar appears
   - "Document processed successfully"
   - Can see document in list

4. **After query:**
   - Results appear within 5-10 seconds
   - Citations with links
   - Relevant text excerpts

---

**Current Status:** 
- ‚úÖ Ollama installed and running
- ‚úÖ Models available (llama3.2, qwen2.5:3b, etc)
- ‚úÖ QSM AppImage built (432MB)
- ‚ö†Ô∏è QSM UI not rendering (white screen)

**Next Action:** Run `./debug-qsm.sh` v√† check console!
