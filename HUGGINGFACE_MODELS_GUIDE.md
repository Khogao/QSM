# ğŸ¤— HuggingFace Model Integration Guide

## âœ¨ Features

### Auto-Discovery
- **Smart Search**: Automatically finds best models for RAG/Document Q&A
- **Filtering**: By size, quantization, downloads, likes
- **Recommendations**: AI-powered suggestions based on your hardware

### Hardware Optimization
- **CPU**: AMD 5700X optimized (your current PC)
- **GPU**: AMD RX 580 8GB Vulkan support
- **NPU**: DirectML for Intel/AMD with NPU (future customer PCs)

### Supported Providers
1. **LM Studio + Vulkan** (â­ Best for you!)
2. **Vulkan Bare Metal** (Experimental)
3. **Ollama** (Fallback)
4. **Cloud**: OpenAI, Gemini, Claude

---

## ğŸ¯ Top Recommended Models (For Your Hardware)

### 1. Tier 1: Best Performance on AMD 5700X CPU

#### Phi-3-Mini-4K-Instruct-GGUF (Q4)
```
Author: microsoft
Size: 2.3 GB
Quantization: Q4_K_M
Context: 4k tokens
Performance: 
  - CPU: 40-50 tokens/sec (very fast)
  - GPU (RX 580): 60-80 tokens/sec
  - NPU: âœ… Supported (DirectML)

Why Recommended:
âœ… Very fast on CPU
âœ… Excellent for Q&A tasks
âœ… Low VRAM (fits easily in RX 580)
âœ… NPU-accelerated (DirectML ready)

HuggingFace: microsoft/Phi-3-mini-4k-instruct-gguf
LM Studio Search: "phi-3-mini"
```

#### TinyLlama-1.1B-Chat-GGUF (Q4)
```
Author: TheBloke
Size: 0.6 GB
Quantization: Q4_K_M
Context: 2k tokens
Performance:
  - CPU: 50-70 tokens/sec (very fast)
  - GPU (RX 580): 80-100 tokens/sec
  - NPU: âœ… Supported

Why Recommended:
âœ… Extremely fast
âœ… Tiny size (600 MB!)
âœ… Good for simple Q&A
âœ… Perfect for testing

HuggingFace: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF
LM Studio Search: "tinyllama"
```

### 2. Tier 2: Balanced Quality + Speed

#### Mistral-7B-Instruct-v0.2-GGUF (Q4)
```
Author: TheBloke
Size: 4.1 GB
Quantization: Q4_K_M
Context: 8k tokens
Performance:
  - CPU: 20-30 tokens/sec (fast)
  - GPU (RX 580): 40-50 tokens/sec
  - NPU: âš ï¸ Partial support

Why Recommended:
âœ… Excellent quality
âœ… Good speed on CPU
âœ… Fits in RX 580 VRAM
âœ… 8k context (longer documents)

HuggingFace: TheBloke/Mistral-7B-Instruct-v0.2-GGUF
LM Studio Search: "mistral-7b-instruct"
```

#### Llama-3-8B-Instruct-GGUF (Q4)
```
Author: QuantFactory
Size: 4.5 GB
Quantization: Q4_K_M
Context: 8k tokens
Performance:
  - CPU: 15-25 tokens/sec (medium-fast)
  - GPU (RX 580): 35-45 tokens/sec
  - NPU: âš ï¸ Partial support

Why Recommended:
âœ… High quality outputs
âœ… Great for complex Q&A
âœ… Meta's latest model
âœ… Well-tested

HuggingFace: QuantFactory/Meta-Llama-3-8B-Instruct-GGUF
LM Studio Search: "llama-3-8b-instruct"
```

### 3. Tier 3: Maximum Quality (Slower)

#### Gemma-2-9B-Instruct-GGUF (Q5)
```
Author: Google
Size: 5.4 GB
Quantization: Q5_K_M
Context: 8k tokens
Performance:
  - CPU: 10-18 tokens/sec (medium)
  - GPU (RX 580): 25-35 tokens/sec
  - NPU: âŒ Not supported

Why Recommended:
âœ… Excellent quality
âœ… Google's model
âœ… Good reasoning
âš ï¸ Slower than others

HuggingFace: google/gemma-2-9b-it-GGUF
LM Studio Search: "gemma-2-9b"
```

### 4. Embedding Models (For RAG)

#### nomic-embed-text-v1.5 (GGUF)
```
Author: nomic-ai
Size: 0.5 GB
Context: 8k tokens
Performance:
  - CPU: Very fast
  - GPU: Ultra fast

Why Recommended:
âœ… Best for embeddings
âœ… Tiny size
âœ… Optimized for RAG
âœ… Supports long context

HuggingFace: nomic-ai/nomic-embed-text-v1.5-GGUF
```

#### all-MiniLM-L6-v2 (ONNX)
```
Author: sentence-transformers
Size: 0.09 GB
Context: 512 tokens
Performance:
  - CPU: Ultra fast
  - NPU: âœ… Supported (ONNX)

Why Recommended:
âœ… Extremely fast
âœ… Tiny model
âœ… NPU-accelerated
âœ… Good quality

HuggingFace: sentence-transformers/all-MiniLM-L6-v2
Note: Already integrated in app (Transformers.js)
```

---

## ğŸš€ Setup Instructions

### Step 1: Install LM Studio
```
1. Download: https://lmstudio.ai/
2. Install and launch
3. Enable Vulkan in Settings:
   - Settings â†’ Hardware
   - â˜‘ Enable GPU acceleration
   - Select: Vulkan
4. Restart LM Studio
```

### Step 2: Download Models via App
```
1. Open QSM app
2. Go to: AI Model Settings (Advanced)
3. Enter HuggingFace token (optional, for faster downloads)
4. Click "Discover Models" button
5. Browse recommended models
6. Click "Download" on desired model
7. Wait for download (will save to LM Studio folder)
8. Model appears in "Local Models" tab
```

### Step 3: Test Connection
```
1. Make sure LM Studio is running
2. In QSM app, click "Test Connection"
3. Should show: "ğŸŸ¢ LM Studio ready (X models loaded)"
4. Select your model from dropdown
5. Ready to query documents!
```

---

## ğŸ’¡ Performance Tips

### For AMD 5700X CPU (Your PC)
```
Best Models:
1. Phi-3-Mini (2.3 GB) - 40-50 tok/s
2. TinyLlama (0.6 GB) - 50-70 tok/s
3. Mistral-7B (4.1 GB) - 20-30 tok/s

Enable:
- â˜‘ Vulkan (1.5-2x speedup via RX 580)
- â˜ NPU (not available on 5700X)
```

### For Customer PCs with NPU (Intel/AMD 2024+)
```
Best Models:
1. Phi-3-Mini (DirectML optimized)
2. TinyLlama (ONNX format)
3. all-MiniLM-L6-v2 (embeddings)

Enable:
- â˜‘ Vulkan
- â˜‘ NPU (DirectML)

Expected Performance:
- 2-3x faster than CPU-only
- Lower power consumption
- Lower heat generation
```

### For AMD RX 580 8GB GPU
```
Max Model Size: 8 GB
Best Quantization: Q4_K_M (best size/quality)
Avoid: Q8, F16 (too large)

Recommended:
- Phi-3-Mini Q4: 2.3 GB âœ…
- Mistral-7B Q4: 4.1 GB âœ…
- Llama-3-8B Q4: 4.5 GB âœ…
- Gemma-2-9B Q5: 5.4 GB âœ…

Too Large:
- Llama-3-70B: 40 GB âŒ
- Any F16/F32 model âŒ
```

---

## ğŸ“Š Model Comparison Table

| Model | Size | CPU Speed | GPU Speed | NPU | Quality | Best For |
|-------|------|-----------|-----------|-----|---------|----------|
| TinyLlama-1.1B | 0.6 GB | â­â­â­â­â­ | â­â­â­â­â­ | âœ… | â­â­â­ | Testing, Simple Q&A |
| Phi-3-Mini | 2.3 GB | â­â­â­â­â­ | â­â­â­â­â­ | âœ… | â­â­â­â­ | Best Overall |
| Mistral-7B | 4.1 GB | â­â­â­â­ | â­â­â­â­ | âš ï¸ | â­â­â­â­â­ | Complex Q&A |
| Llama-3-8B | 4.5 GB | â­â­â­ | â­â­â­â­ | âš ï¸ | â­â­â­â­â­ | High Quality |
| Gemma-2-9B | 5.4 GB | â­â­â­ | â­â­â­ | âŒ | â­â­â­â­â­ | Maximum Quality |

Speed Legend:
- â­â­â­â­â­: >40 tok/s
- â­â­â­â­: 20-40 tok/s
- â­â­â­: 10-20 tok/s

---

## ğŸ”§ Troubleshooting

### LM Studio Not Detected
```
Problem: "LM Studio not running" error

Solutions:
1. Make sure LM Studio is open
2. Check LM Studio is running local server:
   - LM Studio â†’ Developer â†’ Start Server
   - Default: http://localhost:1234
3. Check firewall (allow port 1234)
4. Restart LM Studio
```

### Model Download Fails
```
Problem: "Download failed" error

Solutions:
1. Check HuggingFace token is valid
2. Check internet connection
3. Try downloading directly in LM Studio:
   - LM Studio â†’ Search
   - Type model name
   - Click download
4. Check disk space (models are 2-5 GB each)
```

### Vulkan Not Working
```
Problem: No GPU acceleration

Solutions:
1. Update AMD drivers:
   - https://www.amd.com/en/support
   - Download latest for RX 580
2. Check Vulkan installed:
   - vulkaninfo (in cmd)
3. Enable in LM Studio:
   - Settings â†’ Hardware â†’ Vulkan
4. Restart LM Studio after changing
```

### Slow Performance
```
Problem: Low tokens/sec

Solutions:
1. Use smaller model:
   - Switch to Phi-3-Mini or TinyLlama
2. Enable Vulkan (RX 580)
3. Close other apps (free RAM)
4. Check CPU usage:
   - Should be 80-100% during inference
5. Lower max tokens setting
```

---

## ğŸ“ API Usage Examples

### Query with RAG Context
```typescript
import { callLLM } from './services/llmService';

const response = await callLLM({
  provider: 'lmstudio',
  model: 'local/phi-3-mini-q4',
  prompt: 'Summarize the payment terms',
  context: [
    'Payment terms: Net 30 days from invoice date...',
    'Late payment penalty: 2% per month...',
    'Accepted methods: Wire transfer, check...'
  ],
  temperature: 0.7,
  maxTokens: 2048,
  stream: true
});

console.log(response.answer);
console.log(`Performance: ${response.tokensPerSecond.toFixed(1)} tok/s`);
```

### Stream Response (Word-by-Word)
```typescript
import { streamLLM } from './services/llmService';

for await (const chunk of streamLLM({
  provider: 'lmstudio',
  model: 'local/phi-3-mini-q4',
  prompt: 'What is this document about?',
  context: ragChunks,
  stream: true
})) {
  console.log(chunk.text); // Print each word
  if (chunk.done) {
    console.log(`Total tokens: ${chunk.tokens}`);
  }
}
```

---

## ğŸ¯ Recommended Setup (Complete)

```
Your PC (AMD 5700X + RX 580 8GB):
â”œâ”€ LM Studio (installed) âœ…
â”œâ”€ Vulkan enabled âœ…
â”œâ”€ Models downloaded:
â”‚  â”œâ”€ Phi-3-Mini-4K-Instruct-Q4 (2.3 GB) â† Primary
â”‚  â”œâ”€ Mistral-7B-Instruct-Q4 (4.1 GB) â† Backup
â”‚  â””â”€ nomic-embed-text (0.5 GB) â† Embeddings
â””â”€ Settings:
   â”œâ”€ Provider: LM Studio + Vulkan
   â”œâ”€ Hardware: â˜‘ Vulkan, â˜ NPU
   â”œâ”€ Temperature: 0.7
   â””â”€ Max Tokens: 2048

Customer PCs (Future - Intel/AMD with NPU):
â”œâ”€ LM Studio
â”œâ”€ DirectML enabled
â”œâ”€ Models: Same as above (NPU-optimized)
â””â”€ Settings:
   â”œâ”€ Provider: LM Studio + Vulkan
   â”œâ”€ Hardware: â˜‘ Vulkan, â˜‘ NPU (DirectML)
   â””â”€ Expected: 2-3x faster than your PC
```

---

## ğŸ” Security Notes

- âœ… HuggingFace token is **hashed** (SHA256) in code
- âœ… Raw token stored only in localStorage (never git)
- âœ… Token validated on first use
- âŒ Never commit raw token to git!
- ğŸ”’ Add token to `.gitignore` if stored in file

---

**Ready to use!** Your app now has intelligent model discovery and recommendation. ğŸš€
