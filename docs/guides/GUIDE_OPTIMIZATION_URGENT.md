# ðŸ“‹ HÆ¯á»šNG DáºªN Tá»I Æ¯U MODEL - YOLO MODE

**QUAN TRá»ŒNG**: Model Ä‘ang cháº¡y á»Ÿ **7.2 tok/s** - cáº§n tÄƒng lÃªn **80-100 tok/s**

---

## ðŸš¨ HÃ€NH Äá»˜NG NGAY (5 PHÃšT)

### BÆ°á»›c 1: Má»Ÿ LM Studio
```
1. Click vÃ o icon LM Studio trÃªn taskbar
2. Chá» app hiá»‡n ra
```

### BÆ°á»›c 2: Check GPU Acceleration
```
1. Click "Settings" (icon bÃ¡nh rÄƒng, gÃ³c trÃªn pháº£i)
2. Click tab "Hardware" 
3. TÃ¬m má»¥c "GPU Acceleration"
4. Pháº£i tháº¥y: "Vulkan (AMD)" hoáº·c "ROCm"
5. Náº¿u tháº¥y "None" hoáº·c "CPU":
   - Click dropdown
   - Chá»n "Vulkan" hoáº·c "AMD"
   - Click "Save"
```

### BÆ°á»›c 3: Set GPU Layers
```
1. Quay láº¡i tab "Models" (trÃ¡i gÃ³c)
2. Click vÃ o model "chatbot_vietnamese_law_qwen" Ä‘ang load
3. Click "Settings" cá»§a model
4. TÃ¬m "GPU Layers" hoáº·c "GPU Offload"
5. Äáº·t = 99 (hoáº·c max number)
6. Click "Apply" hoáº·c "Save"
```

### BÆ°á»›c 4: Giáº£m Context
```
Trong cÃ¹ng Settings cá»§a model:
1. TÃ¬m "Context Length" hoáº·c "Max Context"
2. Äáº·t = 4096 (hoáº·c 8192)
3. Click "Apply"
```

### BÆ°á»›c 5: Reload Model
```
1. Click "Unload Model"
2. Äá»£i 3-5 giÃ¢y
3. Click "Load Model" láº¡i
4. Äá»£i model load (30-60 giÃ¢y)
```

---

## âœ… KIá»‚M TRA NGAY

### Test láº¡i tá»‘c Ä‘á»™:
```powershell
cd D:\Work\Coding\QSM
python test_model_streaming.py
```

**Káº¿t quáº£ mong Ä‘á»£i**:
- âœ… Speed: 60-100 tokens/sec
- âœ… Time: ~5-10 seconds (thay vÃ¬ 41s)

**Náº¿u váº«n cháº­m**:
- Thá»­ unload vÃ  download Q4_K_M (4.8 GB)
- Check Task Manager: VRAM usage ~4-5 GB

---

## ðŸŽ¯ SAU KHI Tá»I Æ¯U XONG

### Test vá»›i QSM App:
```
1. Má»Ÿ browser: http://localhost:5173
2. Click "Advanced Query" tab
3. Chá»n "LM Studio" provider
4. Model: chatbot_vietnamese_law_qwen
5. Há»i: "Luáº­t Ä‘áº§u tÆ° 2020 cÃ³ gÃ¬ quan trá»ng?"
6. Xem streaming response
```

---

## ðŸ“ž Náº¾U Cáº¦N Há»– TRá»¢

Cháº¡y lá»‡nh nÃ y vÃ  gá»­i káº¿t quáº£:
```powershell
cd D:\Work\Coding\QSM
python test_model_streaming.py > model_test_result.txt 2>&1
```

