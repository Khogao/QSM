# ğŸ¤– LOCAL LLM TEXT RESTRUCTURING - SETUP GUIDE

## ğŸ¯ **Táº I SAO DÃ™NG LOCAL LLM?**

âœ… **MIá»„N PHÃ** - KhÃ´ng tá»‘n tiá»n API  
âœ… **Báº¢O Máº¬T** - TÃ i liá»‡u khÃ´ng rá»i mÃ¡y  
âœ… **NHANH** - KhÃ´ng delay network  
âœ… **OFFLINE** - Hoáº¡t Ä‘á»™ng khÃ´ng cáº§n internet  

---

## ğŸ“‹ **YÃŠU Cáº¦U Há»† THá»NG:**

### **Tá»‘i thiá»ƒu:**
- RAM: 8GB+ (khuyÃªn dÃ¹ng 16GB)
- Disk: 10GB trá»‘ng (cho model cache)
- Python: 3.8+
- OS: Windows 11 / Mac OS / Linux

### **KhuyÃªn dÃ¹ng (nhanh hÆ¡n):**
- GPU: RTX 3060 trá»Ÿ lÃªn (6GB+ VRAM)
- RAM: 16GB+
- SSD: Äá»ƒ cache models

---

## ğŸ¤– **CÃC MODEL ÄÆ¯á»¢C Há»– TRá»¢:**

### **1. Qwen2.5-7B-Instruct** â­ KHUYÃŠN DÃ™NG!
```
Model: Qwen/Qwen2.5-7B-Instruct
Size: ~8GB RAM (4GB vá»›i GPU)
Speed: ~15 tokens/sec (RTX 3060)
Vietnamese: â­â­â­â­â­ (Xuáº¥t sáº¯c!)
```

**Táº¡i sao tá»‘t:**
- Alibaba train vá»›i dá»¯ liá»‡u tiáº¿ng Viá»‡t
- Hiá»ƒu ráº¥t tá»‘t vÄƒn báº£n phÃ¡p lÃ½
- Accuracy cao nháº¥t cho Vietnamese documents
- Model má»›i nháº¥t (Oct 2024)

---

### **2. Llama-3.2-3B-Instruct**
```
Model: meta-llama/Llama-3.2-3B-Instruct
Size: ~4GB RAM
Speed: ~25 tokens/sec
Vietnamese: â­â­â­â­ (Tá»‘t)
```

**Táº¡i sao tá»‘t:**
- Nhá» nháº¥t (chá»‰ 3B parameters)
- Nhanh nháº¥t
- PhÃ¹ há»£p mÃ¡y yáº¿u (4GB RAM)

---

### **3. Phi-3-mini-4k** (Microsoft)
```
Model: microsoft/Phi-3-mini-4k-instruct
Size: ~4GB RAM
Speed: ~20 tokens/sec
Vietnamese: â­â­â­â­ (Tá»‘t)
```

**Táº¡i sao tá»‘t:**
- Microsoft train cho document tasks
- Optimize cho CPU (khÃ´ng cáº§n GPU)
- Context window 4K (Ä‘á»§ cho há»£p Ä‘á»“ng)

---

## ğŸš€ **CÃCH CÃ€I Äáº¶T:**

### **BÆ°á»›c 1: CÃ i dependencies**

```powershell
# Activate venv
cd D:\Work\Coding\QSM
.\python\venv\Scripts\Activate.ps1

# Install LLM packages
pip install transformers torch accelerate psutil

# Hoáº·c vá»›i CUDA (náº¿u cÃ³ GPU NVIDIA):
pip install transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
pip install accelerate psutil
```

---

### **BÆ°á»›c 2: Check há»‡ thá»‘ng**

```powershell
python python/demo_local_llm.py
```

Chá»n option `0` Ä‘á»ƒ check requirements.

**Expected output:**
```
ğŸ” SYSTEM CHECK:

Python: 3.11.x âœ…
PyTorch: 2.x.x âœ…
CUDA: Available (GPU: RTX 3060) âš¡  # Hoáº·c: Not available (will use CPU) ğŸ’»
VRAM: 12.0 GB
Transformers: 4.40.x âœ…
RAM: 16.0 GB âœ… (Can run all models)
```

---

### **BÆ°á»›c 3: Test vá»›i sample**

```powershell
python python/demo_local_llm.py
```

**Chá»n model:**
- `1` - Qwen2.5-7B (tá»‘t nháº¥t cho tiáº¿ng Viá»‡t) â­
- `2` - Llama-3.2-3B (nhanh nháº¥t)
- `3` - Phi-3-mini (tá»‘t cho documents)

**Láº§n Ä‘áº§u cháº¡y:**
- Sáº½ download model (~4-8GB) 
- Máº¥t 2-5 phÃºt
- Model Ä‘Æ°á»£c cache táº¡i: `C:\Users\<user>\.cache\quicord_models\`
- Láº§n sau cháº¡y ngay (khÃ´ng download láº¡i)

---

## ğŸ“ **CÃCH Sá»¬ Dá»¤NG:**

### **Option 1: Standalone script**

```powershell
python python/text_restructure_local.py \
    --model qwen \
    --input "test_ocr_output.txt" \
    --output "restructured.txt" \
    --type contract
```

### **Option 2: Python API**

```python
from text_restructure_local import LocalLLMRestructurer

# Initialize (láº§n Ä‘áº§u sáº½ download model)
restructurer = LocalLLMRestructurer(model_name="qwen")

# Restructure text
ocr_text = "... vÄƒn báº£n OCR vá»›i Ä‘oáº¡n vÄƒn xÃ¡o trá»™n ..."
restructured, metadata = restructurer.restructure(
    ocr_text, 
    doc_type="contract"
)

print(restructured)
```

### **Option 3: Integrate vÃ o Quicord**

TÃ´i sáº½ táº¡o integration module riÃªng Ä‘á»ƒ báº¡n cÃ³ thá»ƒ enable/disable dá»… dÃ ng.

---

## âš¡ **PERFORMANCE:**

| Model | RAM | GPU VRAM | CPU Speed | GPU Speed |
|-------|-----|----------|-----------|-----------|
| **Qwen2.5-7B** | 8GB | 4GB | ~5 tok/s | ~15 tok/s |
| **Llama-3.2-3B** | 4GB | 2GB | ~10 tok/s | ~25 tok/s |
| **Phi-3-mini** | 4GB | 2GB | ~8 tok/s | ~20 tok/s |

**Example timing (1 page contract ~2000 words):**
- Qwen2.5-7B (CPU): ~60 seconds
- Qwen2.5-7B (GPU): ~20 seconds
- Llama-3.2-3B (CPU): ~30 seconds
- Llama-3.2-3B (GPU): ~10 seconds

---

## ğŸ¯ **Káº¾T QUáº¢ DEMO:**

### **Input (OCR interleaved):**
```
tháº¿ cháº¥p báº±ng báº¥t san cua bÃªn thá»© ba

Cá»™ng hoÃ  xÃ£ há»™i chá»§ Viá»‡t Nam

(TÃ i sÃ¡n lÃ  quyÃªn sÆ° dung

Há»¢P Äá»’NG THáº¾ CHáº¤P
Báº±ng quyá»n sá»­ dá»¥ng Ä‘áº¥t...
```

### **Output (Restructured by LLM):**
```
Cá»™ng hoÃ  xÃ£ há»™i chá»§ Viá»‡t Nam

Há»¢P Äá»’NG THáº¾ CHáº¤P
Báº±ng quyá»n sá»­ dá»¥ng Ä‘áº¥t vÃ  tÃ i sáº£n gáº¯n liá»n vá»›i Ä‘áº¥t

(TÃ i sáº£n lÃ  quyá»n sá»­ dá»¥ng...

tháº¿ cháº¥p báº±ng báº¥t sáº£n cá»§a bÃªn thá»© ba
```

**âœ… Äoáº¡n vÄƒn Ä‘Ã£ Ä‘Æ°á»£c sáº¯p xáº¿p láº¡i Ä‘Ãºng thá»© tá»±!**

---

## ğŸ”§ **TROUBLESHOOTING:**

### **Lá»—i: "CUDA out of memory"**
```powershell
# DÃ¹ng CPU thay vÃ¬ GPU
python python/text_restructure_local.py --device cpu ...
```

### **Lá»—i: "No module named 'transformers'"**
```powershell
pip install transformers torch accelerate
```

### **Lá»—i: "Not enough RAM"**
- ÄÃ³ng cÃ¡c app khÃ¡c
- DÃ¹ng model nhá» hÆ¡n (Llama-3.2-3B hoáº·c Phi-3-mini)

### **Download quÃ¡ cháº­m**
- Model download tá»« HuggingFace
- Náº¿u cháº­m, dÃ¹ng mirror hoáº·c download manual:
  - https://huggingface.co/Qwen/Qwen2.5-7B-Instruct
  - Extract vÃ o: `C:\Users\<user>\.cache\quicord_models\`

---

## ğŸ“Š **SO SÃNH: LOCAL LLM vs CLOUD API:**

| TiÃªu chÃ­ | Local LLM | OpenAI/Claude |
|----------|-----------|---------------|
| **Chi phÃ­** | ğŸ†“ Miá»…n phÃ­ | ğŸ’° $0.01-0.03/doc |
| **Tá»‘c Ä‘á»™** | âš¡ 10-60s/doc | âš¡âš¡ 5-15s/doc |
| **Báº£o máº­t** | ğŸ”’ 100% local | â˜ï¸ Upload to cloud |
| **Offline** | âœ… Hoáº¡t Ä‘á»™ng | âŒ Cáº§n internet |
| **Cháº¥t lÆ°á»£ng** | â­â­â­â­ (Very good) | â­â­â­â­â­ (Excellent) |
| **Setup** | âš™ï¸ Cáº§n cÃ i Ä‘áº·t | ğŸš€ DÃ¹ng ngay |

**Káº¾T LUáº¬N:**  
âœ… **Local LLM** = Tá»‘t cho production (miá»…n phÃ­, báº£o máº­t, offline)  
âœ… **Cloud API** = Tá»‘t cho testing (nhanh, khÃ´ng cáº§n setup)

---

## ğŸ¯ **KHUYáº¾N NGHá»Š:**

### **Náº¿u mÃ¡y máº¡nh (16GB RAM + GPU):**
â†’ DÃ¹ng **Qwen2.5-7B** (cháº¥t lÆ°á»£ng cao nháº¥t cho tiáº¿ng Viá»‡t)

### **Náº¿u mÃ¡y yáº¿u (8GB RAM, khÃ´ng GPU):**
â†’ DÃ¹ng **Llama-3.2-3B** (nhá», nhanh, Ä‘á»§ tá»‘t)

### **Náº¿u cáº§n cháº¥t lÆ°á»£ng tá»‘t nháº¥t:**
â†’ DÃ¹ng **OpenAI GPT-4** hoáº·c **Anthropic Claude** (tÃ´i cÅ©ng cÃ³ thá»ƒ implement!)

---

## ğŸ“ **NEXT STEPS:**

1. âœ… **Test demo**: `python python/demo_local_llm.py`
2. â³ **Integrate vÃ o Quicord**: TÃ´i sáº½ táº¡o module tÃ­ch há»£p
3. â³ **Benchmark quality**: So sÃ¡nh accuracy trÃªn real contracts
4. â³ **Add to GUI**: ThÃªm option "AI Restructure" vÃ o interface

**Báº¡n muá»‘n tÃ´i lÃ m bÆ°á»›c nÃ o trÆ°á»›c?**
