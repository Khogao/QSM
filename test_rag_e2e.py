"""
Test End-to-End RAG v·ªõi Vietnamese Law Model
=============================================

Test query ph·ª©c t·∫°p v·ªÅ quy tr√¨nh ƒë·∫ßu t∆∞ tr∆∞·ªõc/sau 1/7/2025
Ki·ªÉm tra:
- Model c√≥ tr·∫£ l·ªùi ƒë∆∞·ª£c kh√¥ng
- C√≥ citations kh√¥ng
- Response quality
- Performance
"""

import requests
import json
import time
from datetime import datetime

# Config
LM_STUDIO_URL = "http://localhost:1234/v1/chat/completions"
MODEL_NAME = "chatbot_vietnamese_law_qwen"

# Test query ph·ª©c t·∫°p
TEST_QUERY = """
Quy tr√¨nh x√©t duy·ªát th·ªß t·ª•c xin thu·∫≠n ch·ªß tr∆∞∆°ng ƒë·∫ßu t∆∞ tr∆∞·ªõc 1/7/2025 v√† sau ng√†y n√†y ·ªü TPHCM c√≥ g√¨ kh√°c nhau?

H√£y ph√¢n t√≠ch chi ti·∫øt:
1. Quy tr√¨nh tr∆∞·ªõc 1/7/2025
2. Quy tr√¨nh sau 1/7/2025
3. C√°c ƒëi·ªÉm kh√°c bi·ªát ch√≠nh
4. T√†i li·ªáu ph√°p l√Ω li√™n quan
"""

# Context m√¥ ph·ªèng (gi·∫£ l·∫≠p RAG retrieval)
MOCK_CONTEXT = """
[NGU·ªíN 1] Lu·∫≠t ƒê·∫ßu t∆∞ 2020 - ƒêi·ªÅu 33:
Th·ªß t·ª•c c·∫•p Gi·∫•y ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω ƒë·∫ßu t∆∞ ƒë·ªëi v·ªõi d·ª± √°n ƒë·∫ßu t∆∞ kh√¥ng thu·ªôc di·ªán quy·∫øt ƒë·ªãnh ch·ªß tr∆∞∆°ng ƒë·∫ßu t∆∞:
1. Nh√† ƒë·∫ßu t∆∞ n·ªôp 01 b·ªô h·ªì s∆° ƒë·ªÅ ngh·ªã c·∫•p Gi·∫•y ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω ƒë·∫ßu t∆∞ cho c∆° quan ƒëƒÉng k√Ω ƒë·∫ßu t∆∞.
2. Trong th·ªùi h·∫°n 15 ng√†y k·ªÉ t·ª´ ng√†y nh·∫≠n ƒë·ªß h·ªì s∆° h·ª£p l·ªá, c∆° quan ƒëƒÉng k√Ω ƒë·∫ßu t∆∞ c·∫•p Gi·∫•y ch·ª©ng nh·∫≠n ƒëƒÉng k√Ω ƒë·∫ßu t∆∞ cho nh√† ƒë·∫ßu t∆∞.

[NGU·ªíN 2] Ngh·ªã ƒë·ªãnh 31/2021/Nƒê-CP - H∆∞·ªõng d·∫´n Lu·∫≠t ƒê·∫ßu t∆∞ 2020:
ƒê·ªëi v·ªõi d·ª± √°n ƒë·∫ßu t∆∞ c√≥ s·ª≠ d·ª•ng ƒë·∫•t, nh√† ƒë·∫ßu t∆∞ ph·∫£i c√≥ vƒÉn b·∫£n ch·∫•p thu·∫≠n v·ªÅ nhu c·∫ßu s·ª≠ d·ª•ng ƒë·∫•t t·ª´ ·ª¶y ban nh√¢n d√¢n c·∫•p t·ªânh tr∆∞·ªõc khi n·ªôp h·ªì s∆° ƒëƒÉng k√Ω ƒë·∫ßu t∆∞.

[NGU·ªíN 3] Quy·∫øt ƒë·ªãnh 123/Qƒê-UBND ng√†y 15/6/2025 c·ªßa UBND TPHCM:
K·ªÉ t·ª´ ng√†y 1/7/2025, √°p d·ª•ng quy tr√¨nh m·ªõi:
- D·ª± √°n d∆∞·ªõi 100 t·ª∑: kh√¥ng c·∫ßn ch·∫•p thu·∫≠n ch·ªß tr∆∞∆°ng
- D·ª± √°n t·ª´ 100-500 t·ª∑: UBND qu·∫≠n/huy·ªán quy·∫øt ƒë·ªãnh
- D·ª± √°n tr√™n 500 t·ª∑: UBND th√†nh ph·ªë quy·∫øt ƒë·ªãnh
- Th·ªùi gian x·ª≠ l√Ω r√∫t ng·∫Øn t·ª´ 15 ng√†y xu·ªëng 10 ng√†y l√†m vi·ªác

[NGU·ªíN 4] Th√¥ng t∆∞ 03/2025/TT-BKH h∆∞·ªõng d·∫´n th·ª±c hi·ªán:
C√°c b∆∞·ªõc trong quy tr√¨nh m·ªõi (sau 1/7/2025):
1. Nh√† ƒë·∫ßu t∆∞ n·ªôp h·ªì s∆° tr·ª±c tuy·∫øn qua C·ªïng d·ªãch v·ª• c√¥ng TPHCM
2. H·ªá th·ªëng t·ª± ƒë·ªông ki·ªÉm tra t√≠nh ƒë·∫ßy ƒë·ªß h·ªì s∆° (1 ng√†y)
3. Ph√≤ng Kinh t·∫ø th·∫©m ƒë·ªãnh (5 ng√†y)
4. UBND ra quy·∫øt ƒë·ªãnh (4 ng√†y)
5. C·∫•p Gi·∫•y ch·ª©ng nh·∫≠n (trong ng√†y)
"""

def test_rag_query():
    """Test RAG v·ªõi context ƒë∆∞·ª£c inject"""
    
    print("=" * 80)
    print("üß™ TEST RAG END-TO-END - VIETNAMESE LAW MODEL")
    print("=" * 80)
    print()
    
    print("üìã Query:")
    print(TEST_QUERY)
    print()
    
    print("üìö Context ƒë∆∞·ª£c cung c·∫•p (m√¥ ph·ªèng RAG retrieval):")
    print(MOCK_CONTEXT[:500] + "...")
    print()
    
    # System prompt v·ªõi context
    system_prompt = f"""B·∫°n l√† chuy√™n gia t∆∞ v·∫•n ph√°p lu·∫≠t Vi·ªát Nam chuy√™n v·ªÅ ƒë·∫ßu t∆∞.

CONTEXT T√ÄI LI·ªÜU (s·ª≠ d·ª•ng ƒë·ªÉ tr·∫£ l·ªùi):
{MOCK_CONTEXT}

H∆Ø·ªöNG D·∫™N:
1. D·ª±a v√†o context tr√™n ƒë·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi
2. Tr√≠ch d·∫´n ngu·ªìn b·∫±ng c√°ch vi·∫øt [NGU·ªíN 1], [NGU·ªíN 2], v.v.
3. Ph√¢n t√≠ch chi ti·∫øt v√† c√≥ c·∫•u tr√∫c
4. N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y n√≥i r√µ
"""
    
    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": TEST_QUERY}
        ],
        "temperature": 0.3,  # Gi·∫£m ƒë·ªÉ c√≥ c√¢u tr·∫£ l·ªùi ch√≠nh x√°c h∆°n
        "max_tokens": 1000,   # TƒÉng ƒë·ªÉ tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß
        "stream": True
    }
    
    print("-" * 80)
    print("ü§ñ Model ƒëang tr·∫£ l·ªùi...")
    print("-" * 80)
    print()
    
    start_time = time.time()
    response_text = ""
    token_count = 0
    
    try:
        with requests.post(LM_STUDIO_URL, json=payload, stream=True, timeout=300) as response:
            response.raise_for_status()
            
            for line in response.iter_lines():
                if line:
                    line_text = line.decode('utf-8')
                    
                    if line_text.startswith('data: '):
                        data_str = line_text[6:]
                        
                        if data_str.strip() == '[DONE]':
                            break
                        
                        try:
                            data = json.loads(data_str)
                            
                            if 'choices' in data and len(data['choices']) > 0:
                                delta = data['choices'][0].get('delta', {})
                                content = delta.get('content', '')
                                
                                if content:
                                    print(content, end="", flush=True)
                                    response_text += content
                                    token_count += 1
                        
                        except json.JSONDecodeError:
                            continue
        
        end_time = time.time()
        elapsed = end_time - start_time
        tokens_per_sec = token_count / elapsed if elapsed > 0 else 0
        
        print("\n")
        print("=" * 80)
        print("üìä K·∫æT QU·∫¢ TEST")
        print("=" * 80)
        print()
        
        print(f"‚è±Ô∏è  Th·ªùi gian: {elapsed:.2f}s")
        print(f"üìù Tokens: ~{token_count}")
        print(f"‚ö° T·ªëc ƒë·ªô: ~{tokens_per_sec:.1f} tokens/sec")
        print()
        
        # Ph√¢n t√≠ch citations
        citations = []
        for i in range(1, 5):
            if f"[NGU·ªíN {i}]" in response_text or f"[ngu·ªìn {i}]" in response_text.lower():
                citations.append(i)
        
        print("üìå ƒê√ÅNH GI√Å:")
        print()
        
        if citations:
            print(f"‚úÖ Citations: C√≥ tr√≠ch d·∫´n {len(citations)} ngu·ªìn: {citations}")
        else:
            print("‚ùå Citations: KH√îNG c√≥ tr√≠ch d·∫´n (model kh√¥ng follow instruction)")
        
        if len(response_text) > 200:
            print("‚úÖ Response length: ƒê·∫ßy ƒë·ªß (>200 chars)")
        else:
            print("‚ö†Ô∏è  Response length: Ng·∫Øn qu√°, c√≥ th·ªÉ thi·∫øu th√¥ng tin")
        
        if "1/7/2025" in response_text or "1.7.2025" in response_text:
            print("‚úÖ Temporal reasoning: Model hi·ªÉu v·ªÅ th·ªùi gian (tr∆∞·ªõc/sau 1/7/2025)")
        else:
            print("‚ö†Ô∏è  Temporal reasoning: Model c√≥ th·ªÉ kh√¥ng ph√¢n t√≠ch ƒë∆∞·ª£c v·ªÅ th·ªùi gian")
        
        if "kh√°c nhau" in response_text or "kh√°c bi·ªát" in response_text:
            print("‚úÖ Comparison: Model c√≥ so s√°nh s·ª± kh√°c bi·ªát")
        else:
            print("‚ö†Ô∏è  Comparison: Model c√≥ th·ªÉ ch∆∞a so s√°nh r√µ r√†ng")
        
        print()
        
        if tokens_per_sec < 20:
            print("‚ö†Ô∏è  C·∫¢NH B√ÅO: T·ªëc ƒë·ªô r·∫•t ch·∫≠m!")
            print("üí° Khuy·∫øn ngh·ªã: Enable GPU trong LM Studio ƒë·ªÉ tƒÉng t·ªëc 10-15x")
            print("   Xem chi ti·∫øt: OPTIMIZATION_GUIDE_URGENT.md")
        elif tokens_per_sec < 50:
            print("‚ö†Ô∏è  T·ªëc ƒë·ªô ch·∫≠m (c√≥ th·ªÉ ƒëang d√πng CPU)")
            print("üí° C√≥ th·ªÉ c·∫£i thi·ªán b·∫±ng GPU acceleration")
        else:
            print("‚úÖ T·ªëc ƒë·ªô t·ªët!")
        
        print()
        
        # K·∫øt lu·∫≠n t·ªïng th·ªÉ
        score = 0
        if citations: score += 3
        if len(response_text) > 200: score += 2
        if "1/7/2025" in response_text or "1.7.2025" in response_text: score += 2
        if "kh√°c nhau" in response_text or "kh√°c bi·ªát" in response_text: score += 2
        if tokens_per_sec > 50: score += 1
        
        print("üéØ T·ªîNG K·∫æT:")
        print(f"   Score: {score}/10")
        
        if score >= 8:
            print("   ‚úÖ RAG ho·∫°t ƒë·ªông T·ªêT!")
        elif score >= 5:
            print("   ‚ö†Ô∏è  RAG ho·∫°t ƒë·ªông CH·∫§P NH·∫¨N ƒê∆Ø·ª¢C, c·∫ßn c·∫£i thi·ªán")
        else:
            print("   ‚ùå RAG c·∫ßn KH·∫ÆC PH·ª§C nhi·ªÅu v·∫•n ƒë·ªÅ")
        
        print()
        print("=" * 80)
        
        # L∆∞u k·∫øt qu·∫£
        result = {
            "timestamp": datetime.now().isoformat(),
            "query": TEST_QUERY,
            "response": response_text,
            "metrics": {
                "time_seconds": elapsed,
                "tokens": token_count,
                "tokens_per_sec": tokens_per_sec
            },
            "analysis": {
                "citations": citations,
                "has_citations": len(citations) > 0,
                "response_length": len(response_text),
                "has_temporal_reasoning": "1/7/2025" in response_text or "1.7.2025" in response_text,
                "has_comparison": "kh√°c nhau" in response_text or "kh√°c bi·ªát" in response_text,
                "score": score
            }
        }
        
        with open("test_rag_result.json", "w", encoding="utf-8") as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        
        print("üíæ K·∫øt qu·∫£ ƒë√£ l∆∞u v√†o: test_rag_result.json")
        print()
        
        return result
        
    except requests.exceptions.Timeout:
        print("\n‚ùå TIMEOUT: Model kh√¥ng tr·∫£ l·ªùi trong 5 ph√∫t")
        print("üí° C√≥ th·ªÉ do:")
        print("   - Model qu√° ch·∫≠m (c·∫ßn GPU)")
        print("   - Query qu√° ph·ª©c t·∫°p")
        print("   - Context qu√° d√†i")
        return None
        
    except Exception as e:
        print(f"\n‚ùå L·ªñI: {e}")
        return None

if __name__ == "__main__":
    test_rag_query()
